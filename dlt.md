# Mathematical Foundations of Deep Learning at RWTH Aachen (Winter 2023-24)

Instructor: Semih Cayci (cayci[at]mathc.rwth-aachen.de)

## Basics of Machine Learning

1. [Concentration inequalities: Chernoff-Hoeffding](https://github.com/semihcayci/deeplearningtheory/blob/17170abf7c283274caf01e2abf2629e0b55d2c1a/2_Concentration%20Inequalities%20for%20Machine%20Learning/1_ChernoffHoeffding.pdf)
2. [Concentration inequalities: Martingale-based bounds](https://github.com/semihcayci/deeplearningtheory/blob/17170abf7c283274caf01e2abf2629e0b55d2c1a/2_Concentration%20Inequalities%20for%20Machine%20Learning/2_Azuma-McDiarmid.pdf)
3. [Basic supervised learning](https://github.com/semihcayci/deeplearningtheory/blob/17170abf7c283274caf01e2abf2629e0b55d2c1a/3_Basics%20of%20(Supervised)%20Learning%20Theory/1_Basic%20Supervised%20Learning.pdf)
4. [Empirical risk minimization](https://github.com/semihcayci/deeplearningtheory/blob/17170abf7c283274caf01e2abf2629e0b55d2c1a/3_Basics%20of%20(Supervised)%20Learning%20Theory/2_ERM.pdf)

[Exercise sheet 0](https://github.com/semihcayci/deeplearningtheory/blob/17170abf7c283274caf01e2abf2629e0b55d2c1a/voluntary-exercise-sheet0.pdf)
[Exercise sheet 1](https://github.com/semihcayci/deeplearningtheory/blob/17170abf7c283274caf01e2abf2629e0b55d2c1a/Assignment1.pdf)

## Optimization for Deep Learning

This book is on the non-asymptotic theory of high-dimensional statistics. It starts with an elegant treatment of tail and concentration bounds, and covers modern statistical subjects: non-parametric regression, RKHS (reproducible kernel Hilbert spaces), random matrix theory, graphical models, sparse linear models, uniform laws, etc. 

For _concentration bounds_ and _kernel methods_, this book is my favorite.

## Generalization in Deep Learning
### [A. Topics in Non-Parametric Statistics - A. Nemirovski](https://www2.isye.gatech.edu/~nemirovs/Lect_SaintFlour.pdf)

How can you perform statistical inference on infinite-dimensional parameters (e.g., functions, time-dependent signals) from noisy observations? This book provides concise answers to this question by focusing on estimating non-parametric regression to functions and functionals. It is a bit difficult to read because of the notation, but it seems to be highly insightful and self-contained.

### [B. Statistical Inference via Convex Optimization - A. Juditsky and A. Nemirovski](https://www2.isye.gatech.edu/~nemirovs/StatOptNoSolutions.pdf)

It is a brand new book due April 2020 on statistical inference. The subjects include (2A) and extend to signal recovery, (sequential) hypothesis testing and sparse recovery.

## Universal Approximation Theorems for Deep Learning

Classical book on large sample properties and approximations of statistical tests, estimators and procedures.
