# Mathematical Foundations of Deep Learning at RWTH Aachen (Winter 2023-24)

Instructor: Semih Cayci (cayci[at]mathc.rwth-aachen.de)

## Basics of Machine Learning



## Optimization for Deep Learning

This book is on the non-asymptotic theory of high-dimensional statistics. It starts with an elegant treatment of tail and concentration bounds, and covers modern statistical subjects: non-parametric regression, RKHS (reproducible kernel Hilbert spaces), random matrix theory, graphical models, sparse linear models, uniform laws, etc. 

For _concentration bounds_ and _kernel methods_, this book is my favorite.

## Generalization in Deep Learning
### [A. Topics in Non-Parametric Statistics - A. Nemirovski](https://www2.isye.gatech.edu/~nemirovs/Lect_SaintFlour.pdf)

How can you perform statistical inference on infinite-dimensional parameters (e.g., functions, time-dependent signals) from noisy observations? This book provides concise answers to this question by focusing on estimating non-parametric regression to functions and functionals. It is a bit difficult to read because of the notation, but it seems to be highly insightful and self-contained.

### [B. Statistical Inference via Convex Optimization - A. Juditsky and A. Nemirovski](https://www2.isye.gatech.edu/~nemirovs/StatOptNoSolutions.pdf)

It is a brand new book due April 2020 on statistical inference. The subjects include (2A) and extend to signal recovery, (sequential) hypothesis testing and sparse recovery.

## Universal Approximation Theorems for Deep Learning

Classical book on large sample properties and approximations of statistical tests, estimators and procedures.
