<!doctype html>
<html>
<head>
    <title>3_References</title>
    <meta charset="utf-8">
    
</head>
<body>
<h2>References</h2><h3><ul><li><br></li></ul><p></p><p></p><p></p></h3><p dir="ltr" style="text-align: left;"></p><p dir="ltr">We mainly follow Deep Learning Theory Lecture Notes by Matus Telgarsky in this part.&nbsp;https://mjt.cs.illinois.edu/dlt/index.pdf</p><p dir="ltr">The universal approximation result that we covered in the class is based on the following:</p><p dir="ltr"></p><ul><li>Kurt Hornik, Maxwell Stinchcombe, and Halbert White. "Multilayer feedforward networks are universal approximators."&nbsp;<i>Neural networks</i>&nbsp;2, no. 5 (1989): 359-366.<br></li></ul><p>The other widely-used universal approximation results are&nbsp;</p><p></p><ul><li>George Cybenko. "Approximation by superpositions of a sigmoidal function."&nbsp;<i>Mathematics of control, signals and systems</i>&nbsp;2, no. 4 (1989): 303-314.<br></li><li>Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function."&nbsp;<i>Neural networks</i>&nbsp;6, no. 6 (1993): 861-867.<br></li></ul><p>The universal approximation result by Andrew Barron, which is referred to as Barron's universal approximation theorem, is proved in</p><p></p><ul><li>&nbsp;Andrew R. Barron, "Universal approximation bounds for superpositions of a sigmoidal function."&nbsp;<i>IEEE Transactions on Information theory</i>&nbsp;39, no. 3 (1993): 930-945.</li></ul><br><p></p>
</body>
</html>