<!doctype html>
<html>
<head>
    <title>5_References</title>
    <meta charset="utf-8">
    
</head>
<body>
<h2>References</h2><h3></h3>For a classical introduction to Rademacher- and VC-based generalization bounds,<br><ul><li>Shalev-Shwartz, Shai, and Shai Ben-David.&nbsp;<em>Understanding machine learning: From theory to algorithms</em>. Cambridge University Press, 2014.</li></ul><br>For algorithm-dependent generalization bounds:&nbsp;<br><ul><li>Fredrik Hellstr√∂m, Giuseppe Durisi, Benjamin Guedj, and Maxim Raginsky. <em>Generalization bounds: Perspectives from information theory and PAC-Bayes.</em>&nbsp;<span>arXiv preprint arXiv:2309.04381</span>&nbsp;(2023). Link:&nbsp;<a href="https://arxiv.org/pdf/2309.04381.pdf">https://arxiv.org/pdf/2309.04381.pdf</a></li><li>Moritz Hardt, Ben Recht, and Yoram Singer. <em>Train faster, generalize better: Stability of stochastic gradient descent.</em> In&nbsp;<span>International Conference on Machine Learning</span>, pp. 1225-1234. PMLR, 2016.<br></li></ul><p>The first reference contains a very insightful introduction to generalization bounds via information-theoretic tools.</p><p><br></p><p>For a very detailed discussion of Rademacher and Gaussian complexities,</p><p></p><ul><li>Michel Ledoux, and Michel Talagrand.&nbsp;<em>Probability in Banach Spaces: isoperimetry and processes</em>. Vol. 23. Springer Science &amp; Business Media, 1991.<br></li><li>Martin J. Wainwright.&nbsp;<em>High-dimensional statistics: A non-asymptotic viewpoint</em>. Vol. 48. Cambridge University Press, 2019.<br></li></ul><p></p>
</body>
</html>